


spark-1.3.0-bin-hadoop2.4/bin/spark-shell --driver-memory 30G --executor-memory 30G 



import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics


val size="10"

val d_train = sc.textFile("spark-train-"+size+"m.csv").map({ line =>
  val vv = line.split(',').map(_.toDouble)
  val label = vv(0) 
  val features = Vectors.dense(vv.slice(1,vv.size)) 
  LabeledPoint(label, features)
})

val d_test = sc.textFile("spark-test-"+size+"m.csv").map({ line =>
  val vv = line.split(',').map(_.toDouble)
  val label = vv(0) 
  val features = Vectors.dense(vv.slice(1,vv.size)) 
  LabeledPoint(label, features)
})

d_train.cache()
d_test.cache()

d_test.count()
val now = System.nanoTime
d_train.count()
( System.nanoTime - now )/1e9




val now = System.nanoTime
val model = new LogisticRegressionWithLBFGS().setIntercept(true).run(d_train)
( System.nanoTime - now )/1e9

// WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
// WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS

// for n=10M:
// WARN MemoryStore: Not enough space to cache rdd_2_148 in memory! 
// w more RAM (250GB) and --driver-memory 200G OK


model.clearThreshold()  // fixes AUC 0.52 -> 0.69 

val scoreAndLabels = d_test.map { point =>
  val score = model.predict(point.features)
  (score, point.label)
}

val metrics = new BinaryClassificationMetrics(scoreAndLabels)
metrics.areaUnderROC()

